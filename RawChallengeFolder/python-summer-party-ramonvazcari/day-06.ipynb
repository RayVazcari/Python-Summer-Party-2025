{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Day 6: Ice Cream Sales Seasonal Performance Assessment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You are a Product Insights Analyst working with the Ben & Jerry's sales strategy team to investigate seasonal sales patterns through comprehensive data analysis. The team wants to understand how temperature variations and unique transaction characteristics impact ice cream sales volume. Your goal is to perform detailed data cleaning and exploratory analysis to uncover meaningful insights about seasonal sales performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import pandas as pd\nimport numpy as np\n\nice_cream_sales_data_data = [\n  {\n    \"sale_date\": \"2024-07-05\",\n    \"temperature\": 62,\n    \"product_name\": \"Cherry Garcia\",\n    \"sales_volume\": 23,\n    \"transaction_id\": \"TX0001\"\n  },\n  {\n    \"sale_date\": \"2024-08-15\",\n    \"temperature\": 64,\n    \"product_name\": \"Chunky Monkey\",\n    \"sales_volume\": 26,\n    \"transaction_id\": \"TX0002\"\n  },\n  {\n    \"sale_date\": \"2024-09-25\",\n    \"temperature\": 66,\n    \"product_name\": \"Phish Food\",\n    \"sales_volume\": 29,\n    \"transaction_id\": \"TX0003\"\n  },\n  {\n    \"sale_date\": \"2024-10-05\",\n    \"temperature\": 68,\n    \"product_name\": \"Americone Dream\",\n    \"sales_volume\": 32,\n    \"transaction_id\": \"TX0004\"\n  },\n  {\n    \"sale_date\": \"2024-11-15\",\n    \"temperature\": 70,\n    \"product_name\": \"Chocolate Fudge Brownie\",\n    \"sales_volume\": 35,\n    \"transaction_id\": \"TX0005\"\n  },\n  {\n    \"sale_date\": \"2024-12-25\",\n    \"temperature\": 72,\n    \"product_name\": \"Half Baked\",\n    \"sales_volume\": 38,\n    \"transaction_id\": \"TX0006\"\n  },\n  {\n    \"sale_date\": \"2025-01-05\",\n    \"temperature\": 74,\n    \"product_name\": \"New York Super Fudge Chunk\",\n    \"sales_volume\": 41,\n    \"transaction_id\": \"TX0007\"\n  },\n  {\n    \"sale_date\": \"2025-02-15\",\n    \"temperature\": 76,\n    \"product_name\": \"Cherry Garcia\",\n    \"sales_volume\": 44,\n    \"transaction_id\": \"TX0008\"\n  },\n  {\n    \"sale_date\": \"2025-03-25\",\n    \"temperature\": 78,\n    \"product_name\": \"Chunky Monkey\",\n    \"sales_volume\": 47,\n    \"transaction_id\": \"TX0009\"\n  },\n  {\n    \"sale_date\": \"2025-04-05\",\n    \"temperature\": 80,\n    \"product_name\": \"Phish Food\",\n    \"sales_volume\": 50,\n    \"transaction_id\": \"TX0010\"\n  },\n  {\n    \"sale_date\": \"2025-05-15\",\n    \"temperature\": 82,\n    \"product_name\": \"Americone Dream\",\n    \"sales_volume\": 53,\n    \"transaction_id\": \"TX0011\"\n  },\n  {\n    \"sale_date\": \"2025-06-25\",\n    \"temperature\": 84,\n    \"product_name\": \"Chocolate Fudge Brownie\",\n    \"sales_volume\": 1000,\n    \"transaction_id\": \"TX0012\"\n  },\n  {\n    \"sale_date\": \"2024-07-05\",\n    \"temperature\": 86,\n    \"product_name\": \"Half Baked\",\n    \"sales_volume\": 59,\n    \"transaction_id\": \"TX0013\"\n  },\n  {\n    \"sale_date\": \"2024-08-15\",\n    \"temperature\": 88,\n    \"product_name\": \"New York Super Fudge Chunk\",\n    \"sales_volume\": 62,\n    \"transaction_id\": \"TX0014\"\n  },\n  {\n    \"sale_date\": \"2024-09-25\",\n    \"temperature\": 90,\n    \"product_name\": \"Cherry Garcia\",\n    \"sales_volume\": 65,\n    \"transaction_id\": \"TX0015\"\n  },\n  {\n    \"sale_date\": \"2024-10-05\",\n    \"temperature\": 61,\n    \"product_name\": \"Chunky Monkey\",\n    \"sales_volume\": 68,\n    \"transaction_id\": \"TX0016\"\n  },\n  {\n    \"sale_date\": \"2024-11-15\",\n    \"temperature\": 63,\n    \"product_name\": \"Phish Food\",\n    \"sales_volume\": 71,\n    \"transaction_id\": \"TX0017\"\n  },\n  {\n    \"sale_date\": \"2024-12-25\",\n    \"temperature\": 65,\n    \"product_name\": \"Americone Dream\",\n    \"sales_volume\": 74,\n    \"transaction_id\": \"TX0018\"\n  },\n  {\n    \"sale_date\": \"2025-01-05\",\n    \"temperature\": 67,\n    \"product_name\": \"Chocolate Fudge Brownie\",\n    \"sales_volume\": 77,\n    \"transaction_id\": \"TX0019\"\n  },\n  {\n    \"sale_date\": \"2025-02-15\",\n    \"temperature\": 105,\n    \"product_name\": \"Half Baked\",\n    \"sales_volume\": 80,\n    \"transaction_id\": \"TX0020\"\n  },\n  {\n    \"sale_date\": \"2025-03-25\",\n    \"temperature\": 71,\n    \"product_name\": \"New York Super Fudge Chunk\",\n    \"sales_volume\": 83,\n    \"transaction_id\": \"TX0021\"\n  },\n  {\n    \"sale_date\": \"2025-04-05\",\n    \"temperature\": null,\n    \"product_name\": \"Cherry Garcia\",\n    \"sales_volume\": 86,\n    \"transaction_id\": \"TX0022\"\n  },\n  {\n    \"sale_date\": \"2025-05-15\",\n    \"temperature\": 75,\n    \"product_name\": \"Chunky Monkey\",\n    \"sales_volume\": 89,\n    \"transaction_id\": \"TX0023\"\n  },\n  {\n    \"sale_date\": \"2025-06-25\",\n    \"temperature\": 77,\n    \"product_name\": \"Phish Food\",\n    \"sales_volume\": 92,\n    \"transaction_id\": \"TX0024\"\n  },\n  {\n    \"sale_date\": \"2024-07-05\",\n    \"temperature\": 79,\n    \"product_name\": \"Americone Dream\",\n    \"sales_volume\": 95,\n    \"transaction_id\": \"TX0025\"\n  },\n  {\n    \"sale_date\": \"2024-08-15\",\n    \"temperature\": 81,\n    \"product_name\": \"Chocolate Fudge Brownie\",\n    \"sales_volume\": 98,\n    \"transaction_id\": \"TX0026\"\n  },\n  {\n    \"sale_date\": \"2024-09-25\",\n    \"temperature\": 83,\n    \"product_name\": \"Half Baked\",\n    \"sales_volume\": 101,\n    \"transaction_id\": \"TX0027\"\n  },\n  {\n    \"sale_date\": \"2024-10-05\",\n    \"temperature\": 85,\n    \"product_name\": \"New York Super Fudge Chunk\",\n    \"sales_volume\": 104,\n    \"transaction_id\": \"TX0028\"\n  },\n  {\n    \"sale_date\": \"2024-11-15\",\n    \"temperature\": 87,\n    \"product_name\": \"Cherry Garcia\",\n    \"sales_volume\": 107,\n    \"transaction_id\": \"TX0029\"\n  },\n  {\n    \"sale_date\": \"2024-12-25\",\n    \"temperature\": 89,\n    \"product_name\": \"Chunky Monkey\",\n    \"sales_volume\": 110,\n    \"transaction_id\": \"TX0030\"\n  },\n  {\n    \"sale_date\": \"2025-01-05\",\n    \"temperature\": 60,\n    \"product_name\": \"Phish Food\",\n    \"sales_volume\": 113,\n    \"transaction_id\": \"TX0031\"\n  },\n  {\n    \"sale_date\": \"2025-02-15\",\n    \"temperature\": 62,\n    \"product_name\": \"Americone Dream\",\n    \"sales_volume\": 116,\n    \"transaction_id\": \"TX0032\"\n  },\n  {\n    \"sale_date\": \"2025-03-25\",\n    \"temperature\": 64,\n    \"product_name\": \"Chocolate Fudge Brownie\",\n    \"sales_volume\": 119,\n    \"transaction_id\": \"TX0033\"\n  },\n  {\n    \"sale_date\": \"2025-04-05\",\n    \"temperature\": 66,\n    \"product_name\": \"Half Baked\",\n    \"sales_volume\": 122,\n    \"transaction_id\": \"TX0034\"\n  },\n  {\n    \"sale_date\": \"2025-05-15\",\n    \"temperature\": 68,\n    \"product_name\": \"New York Super Fudge Chunk\",\n    \"sales_volume\": 125,\n    \"transaction_id\": \"TX0035\"\n  },\n  {\n    \"sale_date\": \"2025-06-25\",\n    \"temperature\": 70,\n    \"product_name\": \"Cherry Garcia\",\n    \"sales_volume\": 128,\n    \"transaction_id\": \"TX0036\"\n  },\n  {\n    \"sale_date\": \"2024-07-05\",\n    \"temperature\": 72,\n    \"product_name\": \"Chunky Monkey\",\n    \"sales_volume\": 1200,\n    \"transaction_id\": \"TX0037\"\n  },\n  {\n    \"sale_date\": \"2024-08-15\",\n    \"temperature\": 74,\n    \"product_name\": \"Phish Food\",\n    \"sales_volume\": 134,\n    \"transaction_id\": \"TX0038\"\n  },\n  {\n    \"sale_date\": \"2024-09-25\",\n    \"temperature\": 76,\n    \"product_name\": \"Americone Dream\",\n    \"sales_volume\": 137,\n    \"transaction_id\": \"TX0039\"\n  },\n  {\n    \"sale_date\": \"2024-10-05\",\n    \"temperature\": 78,\n    \"product_name\": \"Chocolate Fudge Brownie\",\n    \"sales_volume\": 140,\n    \"transaction_id\": \"TX0040\"\n  },\n  {\n    \"sale_date\": \"2024-11-15\",\n    \"temperature\": 80,\n    \"product_name\": \"Half Baked\",\n    \"sales_volume\": 143,\n    \"transaction_id\": \"TX0041\"\n  },\n  {\n    \"sale_date\": \"2024-12-25\",\n    \"temperature\": 82,\n    \"product_name\": \"New York Super Fudge Chunk\",\n    \"sales_volume\": 146,\n    \"transaction_id\": \"TX0042\"\n  },\n  {\n    \"sale_date\": \"2025-01-05\",\n    \"temperature\": 84,\n    \"product_name\": \"Cherry Garcia\",\n    \"sales_volume\": 149,\n    \"transaction_id\": \"TX0043\"\n  },\n  {\n    \"sale_date\": \"2025-02-15\",\n    \"temperature\": 86,\n    \"product_name\": \"Chunky Monkey\",\n    \"sales_volume\": 22,\n    \"transaction_id\": \"TX0044\"\n  },\n  {\n    \"sale_date\": \"2025-03-25\",\n    \"temperature\": 40,\n    \"product_name\": \"Phish Food\",\n    \"sales_volume\": 25,\n    \"transaction_id\": \"TX0045\"\n  },\n  {\n    \"sale_date\": \"2025-04-05\",\n    \"temperature\": 90,\n    \"product_name\": \"Americone Dream\",\n    \"sales_volume\": 28,\n    \"transaction_id\": \"TX0046\"\n  },\n  {\n    \"sale_date\": \"2025-05-15\",\n    \"temperature\": 61,\n    \"product_name\": \"Chocolate Fudge Brownie\",\n    \"sales_volume\": 31,\n    \"transaction_id\": \"TX0047\"\n  },\n  {\n    \"sale_date\": \"2025-06-25\",\n    \"temperature\": 63,\n    \"product_name\": \"Half Baked\",\n    \"sales_volume\": 34,\n    \"transaction_id\": \"TX0048\"\n  },\n  {\n    \"sale_date\": \"2024-07-05\",\n    \"temperature\": 65,\n    \"product_name\": \"New York Super Fudge Chunk\",\n    \"sales_volume\": 37,\n    \"transaction_id\": \"TX0049\"\n  },\n  {\n    \"sale_date\": \"2024-08-15\",\n    \"temperature\": 67,\n    \"product_name\": \"Cherry Garcia\",\n    \"sales_volume\": 40,\n    \"transaction_id\": \"TX0050\"\n  },\n  {\n    \"sale_date\": \"2024-09-25\",\n    \"temperature\": 69,\n    \"product_name\": \"Chunky Monkey\",\n    \"sales_volume\": 43,\n    \"transaction_id\": \"TX0051\"\n  },\n  {\n    \"sale_date\": \"2024-10-05\",\n    \"temperature\": 71,\n    \"product_name\": \"Phish Food\",\n    \"sales_volume\": 46,\n    \"transaction_id\": \"TX0052\"\n  },\n  {\n    \"sale_date\": \"2024-11-15\",\n    \"temperature\": 73,\n    \"product_name\": \"Americone Dream\",\n    \"sales_volume\": 49,\n    \"transaction_id\": \"TX0053\"\n  },\n  {\n    \"sale_date\": \"2024-12-25\",\n    \"temperature\": 75,\n    \"product_name\": \"Chocolate Fudge Brownie\",\n    \"sales_volume\": 52,\n    \"transaction_id\": \"TX0054\"\n  },\n  {\n    \"sale_date\": \"2025-01-05\",\n    \"temperature\": null,\n    \"product_name\": \"Half Baked\",\n    \"sales_volume\": 55,\n    \"transaction_id\": \"TX0055\"\n  },\n  {\n    \"sale_date\": \"2025-02-15\",\n    \"temperature\": 79,\n    \"product_name\": \"New York Super Fudge Chunk\",\n    \"sales_volume\": 58,\n    \"transaction_id\": \"TX0056\"\n  },\n  {\n    \"sale_date\": \"2025-03-25\",\n    \"temperature\": 81,\n    \"product_name\": \"Cherry Garcia\",\n    \"sales_volume\": 61,\n    \"transaction_id\": \"TX0057\"\n  },\n  {\n    \"sale_date\": \"2025-04-05\",\n    \"temperature\": 83,\n    \"product_name\": \"Chunky Monkey\",\n    \"sales_volume\": 64,\n    \"transaction_id\": \"TX0058\"\n  },\n  {\n    \"sale_date\": \"2025-05-15\",\n    \"temperature\": 85,\n    \"product_name\": \"Phish Food\",\n    \"sales_volume\": 67,\n    \"transaction_id\": \"TX0059\"\n  },\n  {\n    \"sale_date\": \"2025-06-25\",\n    \"temperature\": 87,\n    \"product_name\": \"Americone Dream\",\n    \"sales_volume\": 70,\n    \"transaction_id\": \"TX0060\"\n  },\n  {\n    \"sale_date\": \"2024-12-25\",\n    \"temperature\": 89,\n    \"product_name\": \"Chunky Monkey\",\n    \"sales_volume\": 110,\n    \"transaction_id\": \"TX0030\"\n  }\n]\nice_cream_sales_data = pd.DataFrame(ice_cream_sales_data_data)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 1\n\nIdentify and remove any duplicate sales transactions from the dataset to ensure accurate analysis of seasonal patterns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Note: pandas and numpy are already imported as pd and np\n# The following tables are loaded as pandas DataFrames with the same names: ice_cream_sales_data\n# Please print your final result or dataframe\n\n# Load the ice cream sales data\nicsd_df = ice_cream_sales_data.copy()\nprint(icsd_df)\nprint(\"=\" * 100)\nprint()\n\n# Display initial information about the dataframe\nprint(icsd_df.info())\nprint(\"=\" * 100)\nprint()\n\n# Comments: We can see from the dataframe information that there is a total of 61 entries and there are mising values only on the temperature column.\n# Duplicate values are yet to be found \n\n# Finding null values in the dataframe\nmissing_values = icsd_df.isnull().sum()\nprint(\"The number of missing values in each column of the data set is:\");\nprint(missing_values)\nprint()\n\n# Identifying all null values\nall_missing_rows = icsd_df[icsd_df.isnull().any(axis=1)]\nprint(\"All rows with missing values in the data set are:\");\nprint(all_missing_rows)\nprint(\"=\" * 100)\nprint()\n\n# Checking complete duplicate record\nduplicate_record = icsd_df.duplicated().sum()\nprint(\"The number of duplicate records in the data set is:\");\nprint(duplicate_record)\nprint()\n\n# Identifying all duplicate rows including the first occurrence\nall_duplicate_rows = icsd_df[icsd_df.duplicated(keep=False)]\nprint(\"All duplicate rows in the data set are:\");\nprint(all_duplicate_rows)\nprint(\"=\" * 100)\nprint()\n\n# Based on the two results above, we can see that there are two rows with Null Vlues for temperature and one complete duplicate row. \n# Taking into account the dataframe is of 61 records, it seems fitting that it should be 60. So we will determine to remove or \"drop\" the duplicate row.\n\n# Address the null values by replacing them with the mean of the column\nicsd_df['temperature'] = icsd_df[\"temperature\"].fillna(icsd_df[\"temperature\"].median())\nprint(icsd_df.info())\nprint()\n\n# Dropping duplicate rows\nicsd_df = icsd_df.drop_duplicates()\nprint(icsd_df.info())\nprint(\"=\" * 100)\nprint()\n\n# Final cleaned dataframe and answer\ncleaned_icsd_df = icsd_df\nprint(\"The cleaned data set without null values and duplicate records is:\");\nprint(cleaned_icsd_df)\nprint(cleaned_icsd_df.info())\nprint(\"=\" * 100)\nprint()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 2\n\nCreate a pivot table to summarize the total sales volume of ice cream products by month and temperature range.\nUse the following temperature bins where each bin excludes the upper bound but includes the lower bound:\n- Less than 60 degrees\n- 60 to less than 70 degrees\n- 70 to less than 80 degrees\n- 80 to less than 90 degrees\n- 90 to less than 100 degrees\n- 100 degrees or more"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Load the ice cream sales data\nicsd_df = ice_cream_sales_data.copy()\nprint(icsd_df)\nprint(\"=\" * 100)\nprint()\n\n# Display initial information about the dataframe\nprint(icsd_df.info())\nprint(\"=\" * 100)\nprint()\n\n# Comments: We can see from the dataframe information that there is a total of 61 entries and there are mising values only on the temperature column.\n# Duplicate values are yet to be found \n\n# Finding null values in the dataframe\nmissing_values = icsd_df.isnull().sum()\nprint(\"The number of missing values in each column of the data set is:\");\nprint(missing_values)\nprint()\n\n# Identifying all null values\nall_missing_rows = icsd_df[icsd_df.isnull().any(axis=1)]\nprint(\"All rows with missing values in the data set are:\");\nprint(all_missing_rows)\nprint(\"=\" * 100)\nprint()\n\n# Checking complete duplicate record\nduplicate_record = icsd_df.duplicated().sum()\nprint(\"The number of duplicate records in the data set is:\");\nprint(duplicate_record)\nprint()\n\n# Identifying all duplicate rows including the first occurrence\nall_duplicate_rows = icsd_df[icsd_df.duplicated(keep=False)]\nprint(\"All duplicate rows in the data set are:\");\nprint(all_duplicate_rows)\nprint(\"=\" * 100)\nprint()\n\n# Based on the two results above, we can see that there are two rows with Null Vlues for temperature and one complete duplicate row. \n# Taking into account the dataframe is of 61 records, it seems fitting that it should be 60. So we will determine to remove or \"drop\" the duplicate row.\n\n# Address the null values by replacing them with the mean of the column\nicsd_df['temperature'] = icsd_df[\"temperature\"].fillna(icsd_df[\"temperature\"].median())\nprint(icsd_df.info())\nprint()\n\n# Dropping duplicate rows\nicsd_df = icsd_df.drop_duplicates()\nprint(icsd_df.info())\nprint(\"=\" * 100)\nprint()\n\n# Final cleaned dataframe and answer\ncleaned_icsd_df = icsd_df\nprint(\"The cleaned data set without null values and duplicate records is:\");\nprint(cleaned_icsd_df)\nprint(cleaned_icsd_df.info())\nprint(\"=\" * 100)\nprint()\n\n###################################\n\nicsd_q2_df = cleaned_icsd_df.copy()\nprint(icsd_q2_df.head())\nprint(icsd_q2_df.info())\nprint(\"=\" * 100)\nprint()\n\n# First we need to change the sales_date column to datetime format\nicsd_q2_df[\"sale_date\"] = pd.to_datetime(icsd_q2_df[\"sale_date\"], format=\"%Y-%m-%d\")\nprint(icsd_q2_df.info())\nprint(icsd_q2_df.head())\nprint(\"=\" * 100)\nprint()\n\n# Now we create two new columns for month and year. Note that the month will be in string format\nicsd_q2_df['sale_month'] = icsd_q2_df['sale_date'].dt.month_name()\nicsd_q2_df['sale_year'] =icsd_q2_df['sale_date'].dt.year\nprint(icsd_q2_df.head())\nprint(icsd_q2_df.info())\nprint(\"=\" * 100)\nprint()\n\n#Now we have to do a group by on the month and temperature range columns to get the total sales volume\nicsd_q2_df = icsd_q2_df.groupby(['sale_year', 'sale_month', 'temperature']).agg(total_sales_volume = ('sales_volume', 'sum')).sort_values(by=['sale_month', 'sale_year', 'temperature'])\n\n# We reset the index to make sale_year and sale_month columns again instead of index\nicsd_q2_df = icsd_q2_df.reset_index()\nprint(icsd_q2_df)\nprint(\"=\" * 100)\nprint()\n\n# Now we sort the dataframe by year and month in ascending order for better readability\nicsd_q2_df = icsd_q2_df.sort_values(['sale_year','sale_month'], ascending=True)\nprint(icsd_q2_df)\nprint(\"=\" * 100)\nprint()\n\n# Now we create temperature ranges using pd.cut()\n# We define the bins and labels for the temperature ranges using np.inf to cover all possible values above and below certain thresholds\ntemperature_bins = [-np.inf, 60, 70, 80, 90, 100, np.inf]\ntemperature_labels = ['<60', '60-69', '70-79', '80-89', '90-99', '100+']\n\nicsd_q2_df['temperature_range'] = pd.cut(icsd_q2_df['temperature'], bins=temperature_bins, labels=temperature_labels, right=False)\nprint(icsd_q2_df)\nprint(\"=\" * 100)\nprint()\n\n# Now we can create the pivot table\npivot_table = icsd_q2_df.pivot_table(index= ['sale_year', 'sale_month'], columns='temperature_range', values=['total_sales_volume'], observed=True, aggfunc='sum', fill_value=0)\nprint(pivot_table)\nprint(\"=\" * 100)\nprint()\nprint(\"=\" * 100)\nprint()\n\n# Answer: The pivot table summarizing the total sales volume of ice cream products by month and temperature range is displayed above.\nprint(\"The pivot table summarizing the total sales volume of ice cream products by month and temperature range is:\");\nprint(pivot_table)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 3\n\nCan you detect any outliers in the monthly sales volume using the Inter Quartile Range (IQR) method? A month is considered an outlier if falls below Q1 minus 1.5 times the IQR or above Q3 plus 1.5 times the IQR."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Load the ice cream sales data\nicsd_df = ice_cream_sales_data.copy()\nprint(icsd_df)\nprint(\"=\" * 100)\nprint()\n\n# Display initial information about the dataframe\nprint(icsd_df.info())\nprint(\"=\" * 100)\nprint()\n\n# Comments: We can see from the dataframe information that there is a total of 61 entries and there are mising values only on the temperature column.\n# Duplicate values are yet to be found \n\n# Finding null values in the dataframe\nmissing_values = icsd_df.isnull().sum()\nprint(\"The number of missing values in each column of the data set is:\");\nprint(missing_values)\nprint()\n\n# Identifying all null values\nall_missing_rows = icsd_df[icsd_df.isnull().any(axis=1)]\nprint(\"All rows with missing values in the data set are:\");\nprint(all_missing_rows)\nprint(\"=\" * 100)\nprint()\n\n# Checking complete duplicate record\nduplicate_record = icsd_df.duplicated().sum()\nprint(\"The number of duplicate records in the data set is:\");\nprint(duplicate_record)\nprint()\n\n# Identifying all duplicate rows including the first occurrence\nall_duplicate_rows = icsd_df[icsd_df.duplicated(keep=False)]\nprint(\"All duplicate rows in the data set are:\");\nprint(all_duplicate_rows)\nprint(\"=\" * 100)\nprint()\n\n# Based on the two results above, we can see that there are two rows with Null Vlues for temperature and one complete duplicate row. \n# Taking into account the dataframe is of 61 records, it seems fitting that it should be 60. So we will determine to remove or \"drop\" the duplicate row.\n\n# Address the null values by replacing them with the mean of the column\nicsd_df['temperature'] = icsd_df[\"temperature\"].fillna(icsd_df[\"temperature\"].median())\nprint(icsd_df.info())\nprint()\n\n# Dropping duplicate rows\nicsd_df = icsd_df.drop_duplicates()\nprint(icsd_df.info())\nprint(\"=\" * 100)\nprint()\n\n# Final cleaned dataframe and answer\ncleaned_icsd_df = icsd_df\nprint(\"The cleaned data set without null values and duplicate records is:\");\nprint(cleaned_icsd_df)\nprint(cleaned_icsd_df.info())\nprint(\"=\" * 100)\nprint()\n\n###################################\n\nicsd_q2_df = cleaned_icsd_df.copy()\nprint(icsd_q2_df.head())\nprint(icsd_q2_df.info())\nprint(\"=\" * 100)\nprint()\n\n# First we need to change the sales_date column to datetime format\nicsd_q2_df[\"sale_date\"] = pd.to_datetime(icsd_q2_df[\"sale_date\"], format=\"%Y-%m-%d\")\nprint(icsd_q2_df.info())\nprint(icsd_q2_df.head())\nprint(\"=\" * 100)\nprint()\n\n# Now we create two new columns for month and year. Note that the month will be in string format\nicsd_q2_df['sale_month'] = icsd_q2_df['sale_date'].dt.month_name()\nicsd_q2_df['sale_year'] =icsd_q2_df['sale_date'].dt.year\n\nmonth_year_icsd_df = icsd_q2_df\nprint(month_year_icsd_df.head())\nprint(month_year_icsd_df.info())\nprint(\"=\" * 100)\nprint()\n\n#Now we have to do a group by on the month and temperature range columns to get the total sales volume\nmonthy_sales_volume_df = month_year_icsd_df.groupby(['sale_year', 'sale_month', 'temperature']).agg(total_sales_volume = ('sales_volume', 'sum')).sort_values(by=['sale_month', 'sale_year', 'temperature'])\n\n# We reset the index to make sale_year and sale_month columns again instead of index\nicsd_q2_df = monthy_sales_volume_df.reset_index()\nicsd_q2_df\nprint(\"=\" * 100)\nprint()\n\n# Now we sort the dataframe by year and month in ascending order for better readability\nicsd_q2_df = icsd_q2_df.sort_values(['sale_year','sale_month'], ascending=True)\nprint(icsd_q2_df)\nprint(\"=\" * 100)\nprint()\n\n# Now we create temperature ranges using pd.cut()\n# We define the bins and labels for the temperature ranges using np.inf to cover all possible values above and below certain thresholds\ntemperature_bins = [-np.inf, 60, 70, 80, 90, 100, np.inf]\ntemperature_labels = ['<60', '60-69', '70-79', '80-89', '90-99', '100+']\n\nicsd_q2_df['temperature_range'] = pd.cut(icsd_q2_df['temperature'], bins=temperature_bins, labels=temperature_labels, right=False)\nprint(icsd_q2_df)\nprint(\"=\" * 100)\nprint()\n\n# Now we can create the pivot table\npivot_table = icsd_q2_df.pivot_table(index= ['sale_year', 'sale_month'], columns='temperature_range', values=['total_sales_volume'], observed=True, aggfunc='sum', fill_value=0)\nprint(pivot_table)\nprint(\"=\" * 100)\nprint()\nprint(\"=\" * 100)\nprint()\n\n# Answer: The pivot table summarizing the total sales volume of ice cream products by month and temperature range is displayed above.\nprint(\"The pivot table summarizing the total sales volume of ice cream products by month and temperature range is:\");\nprint(pivot_table)\n\n#############################################################\n\nicsd_q3_df = month_year_icsd_df.copy()\nprint(icsd_q3_df)\nprint(\"=\" * 100)\nprint()\n\nprint(icsd_q3_df.head())\nprint(icsd_q3_df.info())\nprint(\"=\" * 100)\nprint()\n\n#Now we have to do a group by on the month and temperature range columns to get the total sales volume\nmonthy_sales_volume_df = icsd_q3_df.groupby(['sale_year', 'sale_month']).agg(total_sales_volume = ('sales_volume', 'sum')).sort_values(by=['sale_month', 'sale_year'])\n# Now we sort the dataframe by year and month in ascending order for better readability\nmonthy_sales_volume_df = monthy_sales_volume_df.sort_values(['sale_year','sale_month'], ascending=True)\n\n# We reset the index to make sale_year and sale_month columns again instead of index\nicsd_q3_df = monthy_sales_volume_df.reset_index()\nprint(icsd_q3_df)\nprint(\"=\" * 100)\nprint()\n\n# # Boxplot to visualize outliers in total_sales_volume\n# icsd_q3_df.boxplot(column='total_sales_volume', grid=False)\n# print(plt.show())\n\n# From this boxplot we can see there are what appears to be two outliers. Lets check which ones would be\n\n# Check for outliers in continuous variables\noutliers = icsd_q3_df['total_sales_volume'].describe()\nprint(\"\\nSummary of the numerical features, including outliers:\")\nprint(outliers)\nprint(\"=\" * 100)\nprint()\n\n# Calculating IQR and bounds for outlier detection\n# IQR method to detect outliers\n# IQE = Q3 - Q1\nQ1 = icsd_q3_df['total_sales_volume'].quantile(0.25)\nQ3 = icsd_q3_df['total_sales_volume'].quantile(0.75)\nIQR = Q3 - Q1\nprint(\"\\nThe Interquartile Range (IQR) is:\", IQR)\nprint(\"=\" * 100)\nprint()\n\n# Calculating the lower and upper bounds for outlier detection\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\nprint(\"\\nThe lower bound for detecting outliers is:\", lower_bound)\nprint(\"The upper bound for detecting outliers is:\", upper_bound)\nprint(\"=\" * 100)\nprint()\n\n# Identifying the outlier months based on the calculated bounds\noutlier_months = icsd_q3_df[(icsd_q3_df['total_sales_volume'] < lower_bound) | (icsd_q3_df['total_sales_volume'] > upper_bound)]\nprint(\"\\nThe months with outlier total sales volume are:\");\nprint(outlier_months)\nprint(\"=\" * 100)\nprint()\n\n# Answer: The months with outlier total sales volume are:\nprint(\"The months with outlier total sales volume are:\");\nprint(outlier_months)\n\nprint(\"=\" * 100)\nprint()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Made with ❤️ by [Interview Master](https://www.interviewmaster.ai)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3",
      "mimetype": "text/x-python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}